<!-- <!DOCTYPE html> -->
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Junxuan Li, CS master, ANU College of Engineering & Computer Science</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le styles -->
    <link href="./assets_files/bootstrap.min.css" rel="stylesheet">
    <link href="./assets_files/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="./assets_files/yangqing.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
    <script src="assets/js/html5shiv.js"></script>
    <![endif]-->
    
</head>

<div class="visible-phone" id="blackBar">
    <a href="#top">About</a>
    <!--<a href="#publications">publications</a>-->
    <a href="#publications">Publications</a>
    <a href="#projects">Projects</a>

    <!--<a href="#teaching">Teaching</a>-->
    <a target="_blank"
       href="./assets/CV_JunxuanLi.pdf">CV</a>
</div>

<body>

<div class="container span3 hidden-phone">
    <div id="floating_sidebar" class="span3">
        <!-- We use a fancy nav bar if there is enough space -->
        <!--<hr class="hidden-phone">-->
        <br>
        <ul class="nav nav-list bs-docs-sidenav hidden-phone">
            <li><a href="#top">About</a></li>
            <!--<li><a href="#publications">Publications</a></li>-->
            <li><a href="#publications">Publications</a></li>
            <li><a href="#projects">Projects</a></li>
            <!--<li><a href="#teaching">Teaching</a></li>-->
            <li><a target="_blank"
                   href="./assets/CV_JunxuanLi.pdf">CV</a>
            </li>
        </ul>
        <hr class="hidden-phone">
        <div class="text-center hidden-phone">
            <img src="assets_files/me.jpg" alt="photo" class="logo-image">
            <br><br>
            u5990546 AT anu.edu.au <br> Junxuan.Li AT anu.edu.au <br>
        </div>

        <!-- Otherwise, we simply use a flat list of links -->

    </div>
</div>


<div class="container">

    <div class="row">

        <div class="span9">
            <br>
            <h3>
                Junxuan Li (李俊萱)
            </h3>
            <h5>
                Junxuan.Li AT anu.edu.au </a>
            </h5>
            <!-- Do I want to show a pic on the phone screen?
            <div class="text-center visible-phone">
                <img src="assets/img/Yihui.png" alt="photo" width="150px"/>
            </div>
            -->
            <a class="visible-phone pull-left" href="http://daggerfs.com/index.html#">
                <img class="media-object" src="assets_files/me.jpg" width="96px" style="margin: 0px 10px">
            </a>
            <p>
                I'm a PhD student in <a href="https://en.wikipedia.org/wiki/Australian_National_University">The Australian National University</a>, with my interest focus on Computer Vision and Deep Learning, supervised by <a target="_blank" href="http://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a>, <a target="_blank" href="http://users.cecs.anu.edu.au/~arobkell/">Antonio Robles-Kelly</a>, <a target="_blank" href="http://users.cecs.anu.edu.au/~shaodi.you/">Shaodi You</a> and <a target="_blank" href="http://www-infobiz.ist.osaka-u.ac.jp/en/member/matsushita/">Yasuyuki Matsushita</a>. 
            </p>
            <p>
                Previously, I received the Master degree in Australian National University, major in master of computing, in 2018. And B. Eng degree in <a href="https://en.wikipedia.org/wiki/Shanghai_Jiao_Tong_University">Shanghai Jiaotong University</a> in 2016, under the supervision of <a href="https://www.researchgate.net/profile/De_Cheng_Wan"> Decheng Wang</a>.
            </p>

            <p>Here's my <a target="_blank"
                            href="./assets/CV_JunxuanLi.pdf">CV</a>.
            </p>


            <!--
             *** Research ***
            -->
            <!--<h3>-->
            <!--<a name="research"></a> Research-->
            <!--</h3>-->
            <!--<p>-->
            <!--My current research topics include:-->
            <!--</p><ul>-->
            <!--<li> Learning better structures for image feature extraction.-->
            <!--</li><li> Explaining human generalization behavior with visually grounded cogscience models.-->
            <!--</li><li> Making large-scale vision feasible and affordable.-->
            <!--</li></ul>-->
            <!--<p></p>-->
            <!--<p> (Most recent publications to be added) </p>-->


            <!--
             *** Publications ***
            -->
            <h3>
                <a name="publications"></a> Publications
            </h3>
        
        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/neural_ps.png" width="200px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Neural Photometric Stereo for Shape and Material Estimation
                     </strong><br>
                        <strong>Junxuan Li</strong>, and Hongdong Li. <i>Submitted for review. 2021 May.</i>                               
                    </p>
                    <p class="abstract-text">
<!-- This paper addresses a challenging Photometric-Stereo problem where the object to be reconstructed has unknown, non-Lambertian, and possibly spatially-varying surface materials. This problem becomes even more challenging when the shape of the object is highly complex so that shadows cast on the surface are inevitable.  To overcome these challenges, we propose a simple coordinate-based deep MLP (multilayer perceptron) neural network to parameterize both the unknown 3D shape and the unknown spatially-varying reflectance at every image pixel. This network is able to leverage the observed specularities and shadows on the surface, and recover both surface shape, normal and generic non-Lambertian reflectance via an inverse differentiable rendering process.  We explicitly predict cast shadows, mitigating possible artifacts on these shadowing regions, leading to higher estimation accuracy. Our framework is entirely self-supervised, in the sense that it requires neither ground truth shape nor known svBRDF. Tests on real-world images demonstrate that our method achieves state-of-the-art accuracy in both shape recovery and material estimation. Thanks to the small size of the MLP-net, our method is also an order of magnitude faster than previous competing deep-learning based photometric stereo methods. -->
                    </p>
                </div>
            </div> 




        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/neural_plenoptic.jpg" width="200px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Neural Plenoptic Sampling: Capture Light-field from Imaginary Eyes
                     </strong><br>
                        <strong>Junxuan Li</strong>, Yujiao Shi, and Hongdong Li. <i>Submitted for review. 2021 March.</i>                               
                    </p>
                    <p class="abstract-text">
<!-- The Plenoptic function describes the light rays observed from any given position in every viewing direction. It is often parameterized as a 5-D function L(x, y, z, theta, phi) for a static scene.  Capturing all the plenoptic functions in the space of interest is paramount for Image-Based Rendering (IBR) and Novel View Synthesis (NVS). It encodes a complete light-field (i.e., lumigraph) therefore allows one to freely roam in the space and view the scene from any location in any direction.  However, achieving this goal by conventional light-field capture technique is expensive, requiring densely sampling the ray space using arrays of cameras or lenses. This paper proposes a much simpler solution to address this challenge by using only a small number of sparsely configured camera views as input.  Specifically, we adopt a simple Multi-Layer Perceptron (MLP) network as a universal function approximator to learn the plenoptic function at every position in the space of interest. By placing virtual viewpoints (dubbed 'imaginary eyes') at thousands of randomly sampled locations and leveraging multi-view geometric relationship, we train the MLP to regress the plenoptic function for the space.  Our network is trained on a per-scene basis, and the training time is relatively short (in the order of tens of minutes). When the model is converged, we can freely render novel images. Extensive experiments demonstrate that our method well approximates the complete plenoptic function and generates high-quality results. -->
                    </p>
                </div>
            </div> 



        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/cvpr21.jpg" width="200px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Lighting, Reflectance and Geometry Estimation from 360° Panoramic Stereo
                     </strong><br>
                        <strong>Junxuan Li</strong>, Hongdong Li, and Yasuyuki Matsushita. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (CVPR) 2021.</i>
                        <a target="_blank"
                           href="./assets/Lighting Reflectance and Geometry Estimation from 360 Panoramic Stereo.pdf">[pdf]</a>
                        <a target="_blank"
                           href="./assets/Lighting Reflectance and Geometry Estimation from 360 Panoramic Stereo-supp.zip">[supplementary]</a>
                        <a target="_blank"
                           href="https://github.com/junxuan-li/LRG_360Panoramic">[code]</a>                               
                    </p>
                    <p class="abstract-text">
We propose a method for estimating high-definition spatially-varying lighting, reflectance, and geometry of a scene from 360° stereo images. Our model takes advantage of the 360° input to observe the entire scene with geometric detail, then jointly estimates the scene’s properties with physical constraints. We first reconstruct a near-field environment light for predicting the lighting at any 3D location within the scene. Then we present a deep learning model that leverages the stereo information to infer the reflectance and surface normal. Lastly, we incorporate the physical constraints between lighting and geometry to refine the reflectance of the scene. Both quantitative and qualitative experiments show that our method, benefiting from the 360° observation of the scene, outperforms prior state-of-the-art methods and enables more augmented reality applications such as mirror-objects insertion.
                    </p>
                </div>
            </div> 

        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/cvpr19.png" width="200px" height="165px">
                    <p></p>
                    <img class="media-object" src="./assets_files/cvpr19-2.png" width="200px" height="165px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Learning to Minify Photometric Stereo
                     </strong><br>
                        <strong>Junxuan Li</strong>, Antonio Robles-Kelly, Shaodi You, and Yasuyuki Matsushita. <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (CVPR) 2019.</i>
                        <a target="_blank"
                           href="./assets/Learning to Minify Photometric Stereo.pdf">[pdf]</a>
                        <a target="_blank"
                           href="https://github.com/junxuan-li/Learning-to-Minify-Photometric-Stereo">[code]</a>    
                    </p>
                    <p class="abstract-text">
Photometric stereo estimates the surface normal given a set of images acquired under different illumination conditions. To deal with diverse factors involved in the image formation process, recent photometric stereo methods demand a large number of images as input. We propose a method that can dramatically decrease the demands on the number of images by learning the most informative ones under different illumination conditions. To this end, we use a deep learning framework to automatically learn the critical illumination conditions required at input. Furthermore, we present an occlusion layer that can synthesize cast shadows, which effectively improves the estimation accuracy. We assess our method on challenging real-world conditions, where we outperform techniques elsewhere in the literature with a significantly reduced number of light conditions. 
                    </p>
                </div>
            </div> 

        
        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/fftcnn.png" width="200px" height="165px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             A Frequency Domain Neural Network for Fast Image Super-resolution
                     </strong><br>
                        <strong>Junxuan Li</strong>, Shaodi You, and Antonio Robles-Kelly. <i>Neural Networks (IJCNN), 2018 International Joint Conference on. IEEE, 2018. Oral Presentation.</i>
                        <a target="_blank"
                           href="https://arxiv.org/abs/1712.03037">[arXiv]</a>  
                        <a target="_blank"
                           href="./assets/A Frequency Domain Neural Network for Fast Image Super-resolution.pdf">[pdf]</a>   
                        <a target="_blank"
                           href="https://github.com/junxuan-li/A-frequency-domain-neural-network-for-fast-image-super-resolution">[code]</a>                       
                    </p>
                    <p class="abstract-text">
                        In this paper, we present a frequency domain neural network for image super-resolution. The network employs the convolution theorem so as to cast convolutions in the spatial domain as products in the frequency domain. Moreover, the non-linearity in deep nets, often achieved by a rectifier unit, is here cast as a convolution in the frequency domain. This not only yields a network which is very computationally efficient at testing but also one whose parameters can all be learnt accordingly. The network can be trained using back propagation and is devoid of complex numbers due to the use of the Hartley transform as an alternative to the Fourier transform. Moreover, the network is potentially applicable to other problems elsewhere in computer vision and image processing which are often cast in the frequency domain. We show results on super-resolution and compare against alternatives elsewhere in the literature. In our experiments, our network is one to two orders of magnitude faster than the alternatives with an imperceptible loss of performance.
                    </p>
                </div>
            </div> 

        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/srnet.png" width="200px" height="200px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Stereo Super-resolution via a Deep Convolutional Network
                     </strong><br>
                        <strong>Junxuan Li</strong>, Shaodi You, and Antonio Robles-Kelly. <i>Digital Image Computing: Techniques and Applications (DICTA), 2017 International Conference on. IEEE, 2017. Oral Presentation.</i>
                        <a target="_blank"
                           href="./assets/Stereo_Super_resolution_via_a_Deep_ConvolutionalNetwork.pdf">[pdf]</a>                        
                    </p>
                    <p class="abstract-text">
                        In this paper, we present a method for stereo super-resolution which employs a deep network. The network is trained using the residual image so as to obtain a high resolution image from two, low resolution views. Our network is comprised by two deep sub-nets which share, at their output, a single convolutional layer. This last layer in the network delivers an estimate of the residual image which is then used, in combination with the left input frame of the stereo pair, to compute the super-resolved image at output. Each of these sub-networks is comprised by ten weight layers and, hence, allows our network to combine structural information in the image across image regions efficiently. Moreover, by learning the residual image, the network copes better with vanishing gradients and its devoid of gradient clipping operations. We illustrate the utility of our network for image-pair super-resolution and compare our network to its non-gradient trained analogue and alternatives elsewhere in the literature.
                    </p>
                </div>
            </div>      



            <h3>
                <a name="projects"></a> Projects
            </h3>


        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/optship.png" width="200px" height="200px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             The interface development and application of OPT-Ship
                     </strong><br>
                        <strong>Junxuan Li</strong>, Supervisor: Prof. Decheng Wang, <i> 2016</i>                     
                    </p>
                    <p class="abstract-text">
                        This is my undergraduate graduation project thesis. It implemented the interface of a software - OPTShip - by using C++ and Qt platform.
                    </p>
                </div>
            </div>      

        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/prp.png" width="200px" height="200px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Research of mass transit passenger flow distribution base on IC and GPS data
                     </strong><br>
                        <strong>Junxuan Li</strong>, Supervisor: Dr. Linjie Gao, <i> 2011</i>
                    
                    </p>
                    <p class="abstract-text">
                        This is the 26th Participation in Research Program(PRP). This project was aimed to analysis the data retrieved from IC and GPS and give an overall judgment to transit distribution. It was completed by using Python.
                    </p>
                </div>
            </div>   

            <!--<div class="media">-->
            <!--<a class="pull-left" href="#top">-->
            <!--<img class="media-object" src="./assets_files/decaf-features.png" width="96px" height="96px">-->
            <!--</a>-->
            <!--<div class="media-body">-->
            <!--<p class="media-heading">-->
            <!--<strong>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</strong><br>-->
            <!--J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell. arXiv preprint.<br>-->
            <!--<a target="_blank" href="http://arxiv.org/abs/1310.1531">[ArXiv Link]</a>-->
            <!--<a target="_blank" href="http://decaf.berkeleyvision.org/">[Live Demo]</a>-->
            <!--<a target="_blank" href="https://github.com/UCB-ICSI-Vision-Group/decaf-release/">[Software]</a>-->
            <!--<a target="_blank" href="http://www.eecs.berkeley.edu/~jiayq/decaf_pretrained/">[Pretrained ImageNet Model]</a>-->
            <!--</p>-->
            <!--<p class="abstract-text">-->
            <!--We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. We also released the software and pre-trained network to do large-scale image classification.-->
            <!--</p>-->
            <!--</div>-->
            <!--</div>-->

            <!--
             *** Projects ***
            -->
 
            <!-- Footer
            ================================================== -->
            
                </div>
                <div class="row">
                    <div class="span12">
                        <p>
                            modified from <a target="_blank" href="http://yihui-he.github.io/">© Yihui He 2017</a>
                        </p>
                    </div>
                </div>

            </footer>
        </div>
    </div>
</div>
</body>
</html>

<!-- Le javascript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<!--
    <script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
    <script src="assets/js/bootstrap-transition.js"></script>
    <script src="assets/js/bootstrap-alert.js"></script>
    <script src="assets/js/bootstrap-modal.js"></script>
    <script src="assets/js/bootstrap-dropdown.js"></script>
    <script src="assets/js/bootstrap-scrollspy.js"></script>
    <script src="assets/js/bootstrap-tab.js"></script>
    <script src="assets/js/bootstrap-tooltip.js"></script>
    <script src="assets/js/bootstrap-popover.js"></script>
    <script src="assets/js/bootstrap-button.js"></script>
    <script src="assets/js/bootstrap-collapse.js"></script>
    <script src="assets/js/bootstrap-carousel.js"></script>
    <script src="assets/js/bootstrap-typeahead.js"></script>
    <script src="assets/js/bootstrap-affix.js"></script>
    <script src="assets/js/holder/holder.js"></script>
    <script src="assets/js/application.js"></script>
-->
