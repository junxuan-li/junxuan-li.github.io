<!-- <!DOCTYPE html> -->
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Junxuan Li, Research Scientist at Meta</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le styles -->
    <link href="./assets_files/bootstrap.min.css" rel="stylesheet">
    <link href="./assets_files/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="./assets_files/yangqing.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
    <script src="assets/js/html5shiv.js"></script>
    <![endif]-->
    
</head>

<div class="visible-phone" id="blackBar">
    <a href="#top">About</a>
    <!--<a href="#publications">publications</a>-->
    <a href="#experience">Experience</a>
    <a href="#publications">Publications</a>
    <a href="#professional">Professional</a>

    <!--<a href="#teaching">Teaching</a>-->
    <!--
    <a target="_blank"
       href="./assets/CV_JunxuanLi.pdf">CV</a>-->
</div>

<body>

<div class="container span3 hidden-phone">
    <div id="floating_sidebar" class="span3">
        <!-- We use a fancy nav bar if there is enough space -->
        <!--<hr class="hidden-phone">-->
        <br>
        <ul class="nav nav-list bs-docs-sidenav hidden-phone">
            <li><a href="#top">About</a></li>
            <!--<li><a href="#publications">Publications</a></li>-->
            <li><a href="#experience">Experience</a></li>
            <li><a href="#publications">Publications</a></li>
            <li><a href="#professional">Professional</a></li>
            <!--
            <li><a target="_blank"
                   href="./assets/CV_JunxuanLi.pdf">CV</a>
            </li>-->
        </ul>
        <hr class="hidden-phone">
        <div class="text-center hidden-phone">
            <img src="assets_files/me.jpeg" alt="photo" class="logo-image">
            <br><br>
            <a href="mailto:junxuanli@meta.com">Email</a> <br>
            <a href="https://scholar.google.com/citations?user=b2_zvDMAAAAJ&hl=en">Google Scholar</a> <br>
            <a href="https://github.com/junxuan-li">Github</a> <br>
            <a href="https://www.linkedin.com/in/junxuan-li-335421a6/">Linkedin</a> <br>
            <a href="https://twitter.com/JunxuanL">Twitter</a> <br>

        </div>

        <!-- Otherwise, we simply use a flat list of links -->

    </div>
</div>


<div class="container">

    <div class="row">

        <div class="span9">
            <br>
            <h3>
                Junxuan Li <!--(李俊萱)-->
            </h3>
            <h5>
                Research Scientist, Reality Labs Research, Meta </a>
            </h5>
            <!-- Do I want to show a pic on the phone screen?
            <div class="text-center visible-phone">
                <img src="assets/img/Yihui.png" alt="photo" width="150px"/>
            </div>
            -->
            <a class="visible-phone pull-left" href="https://junxuan-li.github.io/">
                <img class="media-object" src="assets_files/me.jpg" width="96px" style="margin: 0px 10px">
            </a>
            <p>
                I'm a Research Scientist at Meta Reality Labs Research, focus on Codec Avatar development. Before that, I was a PhD student in <a href="https://en.wikipedia.org/wiki/Australian_National_University">Australian National University</a>, with my interest focus on Computer Vision and Deep Learning, supervised by <a target="_blank" href="http://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a> and <a target="_blank" href="http://www-infobiz.ist.osaka-u.ac.jp/en/member/matsushita/">Yasuyuki Matsushita</a>. I received the Master degree in Australian National University, major in master of computing, in 2018. And B. Eng degree in <a href="https://en.wikipedia.org/wiki/Shanghai_Jiao_Tong_University">Shanghai Jiaotong University</a> in 2016.
            </p>

            <p>
                My main research topics are computer vision and machine learning. Specifically, I am currently doing researches on 3D object reconstruction, scene reconstruction, and novel view synthesis via deep learning approaches. I am also interested in some AR/VR related tasks, e.g., object insertion and relighting.
            </p>

            <!--
            <p>Here's my <a target="_blank"
                            href="./assets/CV_JunxuanLi.pdf">CV</a>.
            </p>
            -->


            <!--
             *** Research ***
            -->
            <!--<h3>-->
            <!--<a name="research"></a> Research-->
            <!--</h3>-->
            <!--<p>-->
            <!--My current research topics include:-->
            <!--</p><ul>-->
            <!--<li> Learning better structures for image feature extraction.-->
            <!--</li><li> Explaining human generalization behavior with visually grounded cogscience models.-->
            <!--</li><li> Making large-scale vision feasible and affordable.-->
            <!--</li></ul>-->
            <!--<p></p>-->
            <!--<p> (Most recent publications to be added) </p>-->


<hr>
            <h3>
                <a name="experience"></a> Experience
            </h3>


            <div class="media">
                <a class="pull-left">
                        <img class="media-object" src="./assets_files/meta.jfif" width="70px">
                </a>
                    <div class="media-body">
                        <p class="media-heading">
                            <strong>
                                 Research Scientist.
                         </strong> Reality Labs Research, Meta. <br>
                              Pittsburgh, United States. <i>Jul. 2023 -- present</i>
                        </p>
                        <p class="abstract-text">
                            Working on photorealistic telepresence, Codec Avatars and AR/VR.
                        </p>
                    </div>
                </div>


                <div class="media">
                    <a class="pull-left">
                            <img class="media-object" src="./assets_files/tencent.jpeg" width="70px">
                    </a>
                        <div class="media-body">
                            <p class="media-heading">
                                <strong>
                                     Research Intern.
                             </strong> Tencent. <br>
                                  Canberra, Australia. <i>Feb. 2023 -- May 2023</i>
                            </p>
                            <p class="abstract-text">
                                Worked on text-to-3D from diffusion priors.
                            </p>
                        </div>
                    </div>


        <div class="media">
            <a class="pull-left">
                    <img class="media-object" src="./assets_files/meta.jfif" width="70px">
            </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Research Scientist Intern.
                     </strong> Reality Labs Research, Meta. <br>
                          Pittsburgh, United States. <i>Jun. 2022 -- Dec. 2022</i>
                    </p>
                    <p class="abstract-text">
                        Solve problems in enabling photorealistic telepresence, digital human reconstruction, Codec Avatars and AR/VR related tasks using Neural Rendering.
                    </p>
                </div>
            </div>
<hr>

        <div class="media">
            <a class="pull-left">
                    <img class="media-object" src="./assets_files/csiro.jfif" width="70px">
            </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Industrial Trainee.
                     </strong> Data61, CSIRO.<br>
                          Canberra, Australia. <i>Jul. 2016 -- Aug. 2018</i>
                    </p>
                    <p class="abstract-text">
      Worked on computer vision. Finished several research projects during the period: including image super-resolution and optical flow estimation via deep learning.
                    </p>
                </div>
            </div>
<hr>

        <div class="media">
            <a class="pull-left">
                    <img class="media-object" src="./assets_files/anu.jfif" width="70px">
            </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Teaching Assistant.
                     </strong> Australian National University.<br>
                          Canberra, Australia. <i>Sep. 2018 -- Dec. 2022</i>
                    </p>
                    <p class="abstract-text">
            Worked as the tutor of: COMP2400/6240 Relational Database 2018-S2, 2019-S2, 2020-S2, 2021-S2; ENGN4528/6528 Computer Vision 2019-S1; COMP1730/6730 Programming for Scientists 2019-S2; COMP4650/6490 Document Analysis 2019-S2.
                    </p>
                </div>
            </div>
<hr>
            <!--
             *** Publications ***
            -->
            <h3>
                <a name="publications"></a> Publications
            </h3>


        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/megane.gif" width="300px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             MEGANE: Morphable Eyeglass and Avatar Network
                     </strong><br>
                        <strong>Junxuan Li</strong>, Shunsuke Saito, Tomas Simon, Stephen Lombardi, Hongdong Li, and Jason Saragih. <strong><i>CVPR 2023.</i></strong>
                        <a target="_blank"
                           href="https://arxiv.org/abs/2302.04868">[pdf]</a>
                        <a target="_blank"
                           href="https://junxuan-li.github.io/megane/">[Project Page]</a>
                    </p>
                    <p class="abstract-text">
                        We propose a 3D compositional morphable model of eyeglasses that accurately incorporates high-fidelity geometric and photometric interaction effects.<br>
                        We employ a hybrid representation that combines surface geometry and a volumetric representation to enable modification of geometry, lens insertion and frame deformation.<br>
                        Our model is relightable under point lights and natural illumination, which can synthesize casting shadows between faces and glasses.
                    </p>
                </div>
            </div>

<hr>



        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/In-the-wild-Flashlight.gif" width="300px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             In-the-wild Inverse Rendering with a Flashlight
                     </strong><br>
                        Ziang Cheng, <strong>Junxuan Li</strong>, Hongdong Li. <strong><i>CVPR 2023.</i></strong>
                        <a target="_blank"
                           href="https://arxiv.org/abs/2303.14190">[pdf]</a>
                        <a target="_blank"
                           href="https://junxuan-li.github.io/wildlight-website/">[Project Page]</a>
                    </p>
                    <p class="abstract-text">
                        We propose a practical photometric solution for the in-the-wild inverse rendering under unknown ambient lighting. <br>
                        We recovers scene geometry and reflectance using only multi-view images captured by a smartphone. <br>
                        The key idea is to exploit smartphone's built-in flashlight as a minimally controlled light source, and decompose images into two photometric components: a static appearance corresponds to ambient flux, plus a dynamic reflection induced by the flashlight.
                    </p>
                </div>
            </div>

<hr>


        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/apple_combined.gif" width="300px">
                    <img class="media-object" src="./assets_files/helmet_front_left_combined.gif" width="300px">
                    <img class="media-object" src="./assets_files/knight_standing_combined.gif" width="300px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Self-calibrating Photometric Stereo by Neural Inverse Rendering
                     </strong><br>
                        <strong>Junxuan Li</strong>, and Hongdong Li. <strong><i>ECCV 2022.</i></strong>
                        <a target="_blank"
                           href="https://arxiv.org/abs/2207.07815">[pdf]</a>
                        <a target="_blank"
                           href="https://github.com/junxuan-li/SCPS-NIR">[Project Page]</a>
                    </p>
                    <p class="abstract-text">
                        Introduced a self-supervised neural network for uncalibrated photometric stereo problem.<br>
                        The object surface shape, and light sources are jointly estimated via the neural network in an unsupervised manner.
                    </p>
                </div>
            </div>
<hr>

        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/3object_combined.gif" width="300px">
                    <img class="media-object" src="./assets_files/neural_ps.png" width="300px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Neural Reflectance for Shape Recovery with Shadow Handling
                     </strong><br>
                        <strong>Junxuan Li</strong>, and Hongdong Li. <strong><i>CVPR 2022.</i></strong>  <strong style="color:red">Oral presentation</strong>
                        <a target="_blank"
                           href="https://arxiv.org/abs/2203.12909">[pdf]</a>
                        <a target="_blank"
                           href="https://github.com/junxuan-li/Neural-Reflectance-PS">[Project Page]</a>
                    </p>
                    <p class="abstract-text">
                        Formulated the shape estimation and material estimation in a self-supervised framework.<br>
                        Explicitly predicted shadows to mitigate the errors.<br>
                        Achieved the state-of-the-art performance in surface normal estimation and been an order of magnitude faster than previous methods.<br>
                        The proposed neural representation of reflectance also presents higher quality in object relighting task than prior works.
                    </p>
                </div>
            </div> 

<hr>


        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/ACCV22.png" width="300px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Neural Plenoptic Sampling: Learning Light-field from Thousands of Imaginary Eyes
                     </strong><br>
                        <strong>Junxuan Li</strong>, Yujiao Shi, and Hongdong Li. <strong><i>ACCV 2022.</i></strong>
                        <a target="_blank"
                           href="https://openaccess.thecvf.com/content/ACCV2022/papers/Li_Neural_Plenoptic_Sampling_Learning_Light-field_from_Thousands_of_Imaginary_Eyes_ACCV_2022_paper.pdf">[pdf]</a>
                    </p>
                    <p class="abstract-text">
                        Proposed a neural representation for the plenoptic function, which describes light
                        rays observed from any given position in every viewing direction.<br>
                        Proposed proxy depth reconstruction and color-blending network for achieving well
                        approximation on the complete plenoptic function. <br>
                        The generated results are in high-quality with better PSNR than previous methods.
                        The training and testing time of proposed method  is also more than 10 times faster
                        than prior works.
<!--        Keywords: Neural radiance field, novel view synthesis, scene reconstruction.-->
<!-- The Plenoptic function describes the light rays observed from any given position in every viewing direction. It is often parameterized as a 5-D function L(x, y, z, theta, phi) for a static scene.  Capturing all the plenoptic functions in the space of interest is paramount for Image-Based Rendering (IBR) and Novel View Synthesis (NVS). It encodes a complete light-field (i.e., lumigraph) therefore allows one to freely roam in the space and view the scene from any location in any direction.  However, achieving this goal by conventional light-field capture technique is expensive, requiring densely sampling the ray space using arrays of cameras or lenses. This paper proposes a much simpler solution to address this challenge by using only a small number of sparsely configured camera views as input.  Specifically, we adopt a simple Multi-Layer Perceptron (MLP) network as a universal function approximator to learn the plenoptic function at every position in the space of interest. By placing virtual viewpoints (dubbed 'imaginary eyes') at thousands of randomly sampled locations and leveraging multi-view geometric relationship, we train the MLP to regress the plenoptic function for the space.  Our network is trained on a per-scene basis, and the training time is relatively short (in the order of tens of minutes). When the model is converged, we can freely render novel images. Extensive experiments demonstrate that our method well approximates the complete plenoptic function and generates high-quality results. -->
                    </p>
                </div>
            </div>

<hr>


        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/cvpr21.jpg" width="300px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Lighting, Reflectance and Geometry Estimation from 360° Panoramic Stereo
                     </strong><br>
                        <strong>Junxuan Li</strong>, Hongdong Li, and Yasuyuki Matsushita. <strong><i>CVPR 2021.</i></strong>
                        <a target="_blank"
                           href="./assets/Lighting Reflectance and Geometry Estimation from 360 Panoramic Stereo.pdf">[pdf]</a>
                        <a target="_blank"
                           href="./assets/Lighting Reflectance and Geometry Estimation from 360 Panoramic Stereo-supp.zip">[supplementary]</a>
                        <a target="_blank"
                           href="https://github.com/junxuan-li/LRG_360Panoramic">[code]</a>                               
                    </p>
                    <p class="abstract-text">
                        Estimating high-definition spatially-varying lighting (environment map), reflectance, and geometry of a complex indoor scene from a pair of 360° images.<br>
                        Outperformed prior state-of-the-art methods in light estimation (7dB better in PSNR) and geometry estimation.<br>
                        Enabled many augmented reality applications such as mirror-objects insertion.
                    </p>
                </div>
            </div> 
<hr>
        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/cvpr19.png" width="300px" height="200px">
                    <p></p>
                    <img class="media-object" src="./assets_files/cvpr19-2.png" width="300px" height="200px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Learning to Minify Photometric Stereo
                     </strong><br>
                        <strong>Junxuan Li</strong>, Antonio Robles-Kelly, Shaodi You, and Yasuyuki Matsushita. <strong><i>CVPR 2019.</i></strong>
                        <a target="_blank"
                           href="./assets/Learning to Minify Photometric Stereo.pdf">[pdf]</a>
                        <a target="_blank"
                           href="https://github.com/junxuan-li/Learning-to-Minify-Photometric-Stereo">[code]</a>    
                    </p>
                    <p class="abstract-text">
                        Dramatically decrease the demands on the photometric stereo problem by reducing the number of images at input.<br>
                        Automatically learn the critical and informative illuminations required at input.
                    </p>
                </div>
            </div> 

<hr>
        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/fftcnn.png" width="300px" height="200px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             A Frequency Domain Neural Network for Fast Image Super-resolution
                     </strong><br>
                        <strong>Junxuan Li</strong>, Shaodi You, and Antonio Robles-Kelly. <i>Neural Networks (<strong>IJCNN</strong>), 2018 International Joint Conference on. IEEE, 2018. Oral Presentation.</i>
                        <a target="_blank"
                           href="https://arxiv.org/abs/1712.03037">[arXiv]</a>  
                        <a target="_blank"
                           href="./assets/A Frequency Domain Neural Network for Fast Image Super-resolution.pdf">[pdf]</a>   
                        <a target="_blank"
                           href="https://github.com/junxuan-li/A-frequency-domain-neural-network-for-fast-image-super-resolution">[code]</a>                       
                    </p>
                    <p class="abstract-text">
                        A frequency domain neural network for image super-resolution.<br>
                        Employs the convolution theorem so as to cast convolutions in the spatial domain as products in the frequency domain.<br>
                        The network is very computationally efficient at testing, which is one to two orders of magnitude faster than the previous works.                    </p>
                </div>
            </div> 
<hr>
        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/srnet.png" width="300px" height="300px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Stereo Super-resolution via a Deep Convolutional Network
                     </strong><br>
                        <strong>Junxuan Li</strong>, Shaodi You, and Antonio Robles-Kelly. <i>Digital Image Computing: Techniques and Applications (DICTA), 2017 International Conference on. IEEE, 2017. Oral Presentation.</i>
                        <a target="_blank"
                           href="./assets/Stereo_Super_resolution_via_a_Deep_ConvolutionalNetwork.pdf">[pdf]</a>                        
                    </p>
                    <p class="abstract-text">
                        A deep network for images super-resolution with stereo images at input. The network is designed to allow combining structural information in the image across large regions efficiently.<br>
                        By learning the residual image, the network copes better with vanishing gradients and its devoid of gradient clipping operations.
                    </p>
                </div>
            </div>      

<hr>



            <h3>
                <a name="professional"></a> Professional Activities
            </h3>

            <div class="media">
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Reviewer.
                     </strong><br>
                    </p>
                    <p class="abstract-text">
                        Served as the conference reviewer of CVPR, ECCV, ICCV, ICLR, NeurIPS, 3DV and IROS.<br>
                        Served as the reviewer of multiple journals: TPAMI, IJCV, SIGGRAPH and RA-L.
                    </p>
                </div>
            </div>

<hr>
            <!--<div class="media">-->
            <!--<a class="pull-left" href="#top">-->
            <!--<img class="media-object" src="./assets_files/decaf-features.png" width="96px" height="96px">-->
            <!--</a>-->
            <!--<div class="media-body">-->
            <!--<p class="media-heading">-->
            <!--<strong>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</strong><br>-->
            <!--J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell. arXiv preprint.<br>-->
            <!--<a target="_blank" href="http://arxiv.org/abs/1310.1531">[ArXiv Link]</a>-->
            <!--<a target="_blank" href="http://decaf.berkeleyvision.org/">[Live Demo]</a>-->
            <!--<a target="_blank" href="https://github.com/UCB-ICSI-Vision-Group/decaf-release/">[Software]</a>-->
            <!--<a target="_blank" href="http://www.eecs.berkeley.edu/~jiayq/decaf_pretrained/">[Pretrained ImageNet Model]</a>-->
            <!--</p>-->
            <!--<p class="abstract-text">-->
            <!--We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. We also released the software and pre-trained network to do large-scale image classification.-->
            <!--</p>-->
            <!--</div>-->
            <!--</div>-->

            <!--
             *** Projects ***
            -->
 
            <!-- Footer
            ================================================== -->
            
                </div>
                <div class="row">
                    <div class="span12">
                        <p>
                            modified from <a target="_blank" href="http://yihui-he.github.io/">© Yihui He 2017</a>
                        </p>
                    </div>
                </div>

            </footer>
        </div>
    </div>
</div>
</body>
</html>

<!-- Le javascript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<!--
    <script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
    <script src="assets/js/bootstrap-transition.js"></script>
    <script src="assets/js/bootstrap-alert.js"></script>
    <script src="assets/js/bootstrap-modal.js"></script>
    <script src="assets/js/bootstrap-dropdown.js"></script>
    <script src="assets/js/bootstrap-scrollspy.js"></script>
    <script src="assets/js/bootstrap-tab.js"></script>
    <script src="assets/js/bootstrap-tooltip.js"></script>
    <script src="assets/js/bootstrap-popover.js"></script>
    <script src="assets/js/bootstrap-button.js"></script>
    <script src="assets/js/bootstrap-collapse.js"></script>
    <script src="assets/js/bootstrap-carousel.js"></script>
    <script src="assets/js/bootstrap-typeahead.js"></script>
    <script src="assets/js/bootstrap-affix.js"></script>
    <script src="assets/js/holder/holder.js"></script>
    <script src="assets/js/application.js"></script>
-->
